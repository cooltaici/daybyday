# DeepLab: Semantic Image Segmentation withDeep Convolutional Nets, Atrous Convolution,and Fully Connected CRFs
**地址**：https://arxiv.org/pdf/1606.00915.pdf
###摘要
在这项工作中有三个主要贡献。
- 第一，使用上采样滤波器进行卷积，或者将“多孔 convolution”应用在密集预测任务。多孔卷积允许我们在DCNN计算特征响应时明确地控制响应的分辨率。能有效地扩大滤波器的视野以并入更多的上下文而不增加参数的数量或计算量。 
- 第二，提出多孔空间金字塔池化(ASPP)，在多尺度上鲁棒地分割物体。ASPP使用多个采样率和有效视野的滤波器来探测进入的卷积特征层，从多尺度捕获物体以及图像上下文。 
- 第三，通过合并DCNN和概率图模型的方法，增强物体边界的定位。通常在DCNN中采用最大池化和下采样方法实现不变性，但对定位精度有一定的限制。我们通过将最终的DCNN层的响应与完全连接的条件随机场(CRF)相结合来克服这一点，定性和定量地提高定位性能。我们提出的“DeepLab”系统在PASCAL VOC-2012语义图像分割任务中达到了新的最新技术，在测试集中达到79.7％的mIOU，并将结果推广到另外三个数据集:PASCAL-Context，PASCAL-Person Part和Cityscapes。所有代码已经开源。
### 引言
#### DCNN应用于语义图像分割中的三个挑战
&emsp;**挑战一**：是由连续DCNN层中的最大池化和下采样(滑动步长)的重复组合引起的。当DCNN以完全卷积方式使用时，会导致特征图的空间分辨率显著降低。</br>
&emsp;**挑战二**：是由于不同scale物体的存在。通常的做法是对同样的图片，采用不同scale的DCNN版本，但这样做的效率太低了。我们提出了一种高效地计算方案:在卷积之前以多个采样率重新采样特定的特征层。这相当于用具有互补的有效视野的多个滤波器探测原始图像，从而在多个尺度上捕获物体以及有用的图像上下文。与实际重新采样特征不同，我们有效地使用具有不同采样率的多个并行的多孔卷积层来实现该映射，称之为(ASPP)技术。</br>
&emsp;**挑战三**:涉及到以物体为中心的分类器所需要的空间变换的不变性，固有地限制了DCNN的空间精度。一种减轻此问题的方法是当计算最终的分割结果时使用跳跃层(skip-layers)从多个网络层提取“hyper-column”超抽象特征。我们探索出一种替代方法，并证明这种方法是非常有效的。特别地，通过使用全连接的条件随机场(CRF)来提高模型捕获精细细节的能力。CRF广泛用于语义分割，将从像素和边缘或超像素的局部交互中捕获低阶信息的多路分类器计算得到的类分数结合起来。虽然建模分段依赖性[26], [27], [28]和/或分割用的高阶依赖性[29, 30, 31, 32, 33]等通过提高复杂度的工作陆续被提出，我们使用[22]提出的全连接的配对CRF，计算更有效，并能够捕获细微的边缘细节，同时也适应长距离的依赖。[22]中的模型很大程度上提高了基于增强像素级分类器的性能。本文展示了当与基于DCNN的像素级别分类器相结合时，有更好的结果。</br>

![Figure 1](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Figure1.PNG)

&emsp;本文提出的DeepLab基本原理如图1所示。用作分类任务的VGG-16和ResNet-101预训练之后，备用做语义分割任务。(1)把所有全连接层改为全卷积层。(2)用孔洞卷积提高特征分辨率，使得我们能够计算每8个像素计算一个特征，而不是32个像素。然后我们使用双线性插值使得上采样8倍，匹配原输入大小，并作为全连接CRF的输入。
我们的Deeplab有以下几个有点：
- 速度:凭借多孔卷积，我们的密集DCNN在NVidia Titan X GPU上以8 FPS运行，同时全连接CRF的Mean Field Inference在CPU上需要0.5秒
- 准确性:我们在几个具有挑战性的数据集上取得最好结果，包括PASCAL VOC 2012语义分割基准[34]，PASCAL-Context[35]，PASCAL-Person-Part[36]和Cityscapes[37]]
- 简单性:系统由两个非常完善的模块，DCNN和CRF级联组成。
&emsp;本文中提到的DeepLab系统与原始会议中发布的第一个版本[38]相比，具有几个改进。新版本可通过多尺度输入处理[17]，[39]，[40]或者所提出的ASPP技术来更好地分割多尺度的物体。通过采用最新的ResNet [11]图像分类DCNN构建了DeepLab的残差网络变体，与原始基于VGG-16的模型相比，实现了更好的语义分割性能[4]。最后对多个模型变体进行了更全面的实验评估，不仅在PASCAL VOC 2012基准测试中，还针对其他具有挑战性的任务得出了最好的结果。用Caffe框架来实现这些方法[41]。在 http://liangchiehchen.com/projects/DeepLab.html 上分享了代码和模型。
### 2. 相关工作
&emsp;在过去十年中开发的大多数成功的语义分割系统都依赖于手工设计特征的单一分类器(如Boosting[24, 42]，Random Forests[43]或Support Vector Machines[44])的结合。通过从上下文[45]和结构化预测技术[22, 26, 27, 46]中收集更丰富的信息已经实现了显著的改进，但是这些系统的性能一直受到特征的有限表达力的影响。在过去几年中，深度学习从图像分类中的迅速突破转移到了语义分割任务中。由于这个任务涉及分割和分类，所以核心问题是如何组合这两个任务。
&emsp;用于语义分割的基于DCNN的第一类系统通常采用自下而上的级联的图像分割，然后是基于DCNN的区域分类。例如[47]，[48]提供的边界框候选和掩码区域在[7]和[49]中被用作DCNN的输入，整合形状信息给分类过程。类似地，[50]的作者采用超像素表示。即使这些方法可以通过良好的分割得到的尖锐边界受益，但是它们也无法从任何错误中恢复。
&emsp;第二类作品依赖于使用卷积计算的DCNN特征进行密集图像标注，并将它们与独立获得的分割结合在一起。其中[39]首先在多个图像分辨率下应用DCNN，然后采用分割树来平滑预测结果。最近，[21]提出使用skip层并连接DCNN内的中间特征图进行像素分类。此外，[51]建议按局部候选池化中间特征图。这些方法仍然采用从DCNN分类器结果分离的分割算法，因此会过早承担决策风险。
&emsp;第三类作品使用DCNN直接提供密集的类别级的像素标签，以至于可以完全丢弃分割。[14]，[52]的免分割方法直接将DCNN以完全卷积的方式应用于整个图像，将DCNN的最后全连接层转换为卷积层。为了处理引言中概述的空间定位问题，[14]对来自中间特征图的分数进行上采样并连接，[52]通过将粗略结果传播到另一个DCNN，从粗到细优化分预测结果。我们在这些作品的基础上，通过对特征分辨率进行控制来扩展它们，引入多尺度池化技术并将[2密集连接的CRF集成在DCNN之上。我们发现它产生了明显地更好的分割结果，特别是在物体边界处。DCNN和CRF的组合当然不是新的，但以前的工作只尝试了局部连接的CRF模型。具体来说，[53]使用CRF作为基于DCNN的重排系统，而[39]将超像素视为局部配对的CRF节点，并使用图切割进行离散推理。因此，它们的模型受限于超像素计算中的错误或忽略的长距离依赖关系。我们将每个像素看作是由DCNN接收一元势能的CRF节点。重要的是，[22]的全连接CRF模型中的高斯CRF势能可以捕获长距离依赖性，同时该模型适合于快速平均场推理。我们注意到，传统的图像分割任务已经广泛研究了平均场推理[54]，[55]，[56]，但是这些较旧的模型通常限于短距离连接。在某些独立工作中，[57]使用了非常相似的密集连接的CRF模型来改进材料分类问题的DCNN结果。然而，[57]的DCNN模块仅通过稀疏点监督进行训练，而不是每个像素的密集监督。
&emsp;多个团体取得了重大进展，大大提高了PASCAL VOC 2012语义分割基准的水平，反映在基准测试排行榜的高水平论文，如[17]，[40]，[58]，[59]，[60] ]，[61]，[62]，[63]。http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6。有趣的是，大多数表现好的方法都采用了DeepLab系统的一个或两个关键点:多孔卷积，用于通过全连接的CRF进行有效的密集特征提取和细化原始DCNN分数。我们在下面概述一些最重要和有趣的进展。
&emsp;结构化预测的端到端训练最近在几项相关工作中得到探讨。虽然我们使用CRF作为后端处理方法，[40]，[59]，[62]，[64]，[65]已经成功地进行了DCNN和CRF的联合学习。特别地，[59]，[65]展开了CRF均值场推理步骤，将整个系统转换成端对端可训练的前馈网络，而[62]用卷积滤波器近似估计密集CRF平均值场推理的一次迭代。[40]，[66]另一个富有成效的方向追求，通过学习配对的DCNN-CRF条件，以更巨大的计算代价大大提高了性能。在另一个不同的方向，[63]将平均场推断中使用的双边滤波模块替换为更快的域变换模块[67]，提高了整个系统的速度并降低了存储器要求，[18]，[68]将语义分割与边缘检测相结合。
&emsp;基于弱监督的论文有许多，放宽像素级语义标注到整个训练集[58]，[69]，[70]，[71]，得到比弱监督的预训练的DCNN系统更好的结果，如[72]。在另一个研究领域，[49]，[73]追求实体分割，同时处理物体检测和语义分割。
&emsp;我们所说的多孔卷积，最初是为了高效计算[15]中的多孔变换/非抽取小波变换。感兴趣的读者可参考[74]。多孔卷积也与多比例信号处理中的(多采样率系统中用于将抽取器或插值器移动到合适位置)密切相关，多重比例信号处理建立在输入信号和滤波器采样率相互作用上，如[75]。很多作者在DCNNs中使用相同的操作进行更密集的特征提取，如[3]，[6]，[16]。除了简单的增强分辨率，多孔卷积允许我们扩大滤波器的视野，以纳入更多的上下文内容，[38]中已经表明这是有益的。这种方法已经被[76]进一步改进，他们采用一系列采样率不断增大的多孔卷积来聚合多尺度上下文。这里提出的用于捕获多尺度物体和上下文的多孔空间金字塔池化方案，也采用了具有不同采样率的多个多孔卷积层，但是它们是并行排列而不是串行的。有趣的是，在更广泛的任务中，例如物体检测[12]，[77]，实体分割[78]，视觉问答[79]和光流[80]，也采用了多孔卷积技术。
### 3. Methods
#### 孔洞卷积用于密集预测以及大的感受野
&emsp;已经证明能以全卷积方式部署DCNN来简单而成功地解决语义分割或其他密集预测任务[3]，[14]。然而，这些网络上连续层的max-pooling和striding的重复组合显著减小了最终的特征图的空间分辨率，典型地最新的DCNN中沿每个方向32倍减小。[14]的补救方法是使用“解/反卷积”层，但需要额外的内存和时间。
&emsp;我们使用[15]中的多孔变换/非抽取小波变换的高效计算而开发的多孔卷积，并且在DCNN上下文中使用，如[3]，[6]，[16]。该算法允许我们以任何期望的分辨率计算任何层的响应。一旦网络训练完成，它可以应用于post-hoc，也可以与训练无缝集成。
&emsp;首先考虑一维信号，将具有长度为KK的滤波器w[k]w[k]的一维输入信号x[i]x[i]的输出y[i]y[i]定义为: 
![Formular 1](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/DeepLab2_Formular1.PNG)
![Figure 2](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Figure2.PNG)
图2:1-D中多孔卷积图示。(a)在低分辨率输入特征图上具有标准卷积的稀疏特征提取。(b)以比例r=2r=2的多孔卷积的密集特征提取，应用于高分辨率输入特征图。 
![Figure 3](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Figure3.PNG)
图3:2维多孔卷积的图示。顶行:在低分辨率输入特征图上具有标准卷积的稀疏特征提取。底行:比例r=2r=2的多孔卷积的密集特征提取，应用于高分辨率输入特征图。
&emsp;我们通过图3中的一个简单的例子来说明算法在2维中的操作:给定一个图像，我们假设我们首新行将分辨率降低2倍的下采样操作，然后再执行核的卷积，这里是垂直高斯导数滤波器。如果在原始图像坐标中植入了所得到的特征图，仅能在图像位置的1/4处获得响应。相反，如果我们将全分辨率图像与一个带有孔的滤波器进行卷积，我们可以计算所有图像位置的响应，其中我们用因子2的上采样原始滤波器，并在滤波器值之间引入零点。虽然有效滤波器的尺寸增加了，但是我们只需要考虑非零的滤波器值，因此滤波器参数的数量和每个位置的操作数保持不变。所得到的方案允许我们简单且明确地控制神经网络特征响应的空间分辨率。
&emsp;在DCNN的上下文中，可以在层次链中使用多孔卷积，允许我们以任意高分辨率计算最终的DCNN网络响应。例如，为了对VGG-16或ResNet-101网络中计算出的特征响应的空间密度加倍，我们发现最后的池化层或者卷积层降低了分辨率(分别为“pool5”或“conv5_1”)，那么我们设置其步幅为1以避免信号抽取，并用比例r=2r=2的多孔卷积层代替所有后续卷积层。将这种方法推广至整个网络就可以在原始图像分辨率下计算特征响应，但是这样做的代价太高了。我们采用了混合方式，得到了一个高效且准确的平衡，使用因子4的多孔卷积来提高计算的特征图的密度，然后通过另一个因子为8的快速双线性插值在原始图像分辨率上恢复特征图。如图5所示，类别得分图(对应于对数概率)相当平滑，所以双线性插值是足够的，与[14]所采用的解卷积方法不同，所提出的方法将图像分类网络转换为密集特征提取器，无需学习任何额外的参数，从而实现更快的DCNN训练。 
&emsp;多孔卷积还允许我们在任何DCNN层任意放大滤波器的视野。最新的DCNN采用空间小的卷积内核(通常为3×3)，以便保持计算和可控的参数数量。比例rr的多孔卷积在连续滤波器值之间引入r−1个0，有效地将一个k×k的滤波器的核大小扩大到ke=k+(k−1)(r−1)ke=k+(k−1)(r−1)，而不增加参数数量或计算。因此，它提供了一种有效的控制视野的机制，并在准确定位(小视野)和上下文同化(大视野)之间找到最佳权衡。我们成功地采用了这种技术:我们的DeepLab-LargeFOV模型变体[38]在VGG-16 ‘fc6’层采用了比例r=12的多孔卷积，有显著的性能提升，如第4节所述。
&emsp;关于实现，有两种方法来有效的执行多孔卷积。第一个是通过插入空洞(零)来隐含地对滤波器进行上采样，或等效稀疏地对输入特征图进行采样。如[15]在早期工作中，[6]，[38]，[76]在Caffe框架内，通过向im2col函数(从多通道特征图中提取矢量化块)添加稀疏采样底层特征图实现了这一点。第二种方法，最初由[82]提出并在[3]，[16]中使用，它通过用一个与多孔卷积比例rr等效的因子对输入特征图下采样，对每个r×r可能的变化，消除隔行扫描以产生一个r2r2降低的分辨率图。然后将标准卷积应用于这些中间特征图，并隔行扫描生成原始图像分辨率。通过将多孔卷积减少为常规卷积，可以使用现成的高度优化的卷积方法。我们已经在TensorFlow框架中实现了第二种方法。
#### 3.2 多尺度图像表示基于孔洞卷积空间金字塔池化
&emsp;DCNN已经给出显著地表示尺度的能力，只需通过对含有多尺度物体的数据集进行训练。然而，明确的物体尺度可以提高DCNN成功处理大小物体的能力[6]。
&emsp;我们已经尝试了两种方法来处理语义分割中的尺度变换。第一种方法相当于标准多尺度处理[17]，[18]。我们通过共享相同参数的并行DCNN分支从多个原始图像(本文取3个)的不同尺度版本中提取DCNN得分图。为了产生最终的结果，我们对从并行DCNN分支到原始图像分辨率之间的特征图进行双线性插值，并融合它们，在不同尺度上获取每个位置的最大响应。我们在训练和测试期间都这样做。多尺度处理显著提高了性能，但代价是在输入图像的多个尺度上对所有DCNN层计算特征响应。
&emsp;第二种方法受[20]的R-CNN空间金字塔池化方法成功的启发，它指出在任意尺度的区域可以用从单个尺度中重采样提取的卷积特征进行准确有效地分类。我们已经实现了他们的方案的一个变体，用不同采样率的多个并行的多孔卷积层。对每个采样率提取的特征进一步在单独的分支中进行处理，并进行融合以产生最终结果。所提出的“多孔空间金字塔池化”(DeepLab-ASPP)方法泛化了DeepLab-LargeFOV变体，如图4所示。
![Figure 4](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Figure4.PNG) 
图4:多孔金字塔池化(ASPP)。为了分类中心像素(橙色)，ASPP通过采用不同比例的多个并行滤波器开发多尺度特征。视野有效区以不同的颜色显示。
#### 3.3 使用全连接条件随机场进一步预测边界
&emsp;定位精度与分类性能之间的权衡似乎在DCNN中是固有的:具有多个最大池化层的更深层次的模型在分类任务中被证明是非常成功的，然而顶层节点增加的不变性和大的感受野只能产生平滑的响应。如图5所示，DCNN得分图可以预测物体的存在和粗略位置，但不能真正描绘其边界。
&emsp;以前的工作从两个方向来解决这个定位挑战。第一种方法是利用卷积网络中多层信息，以更好地估计物体边界[14]，[21]，[52]。第二个是采用超像素表示，将定位任务委托给低层分割方法[50]。
&emsp;我们通过耦合DCNN的识别能力和全连接的CRF的细粒度定位精度来寻求替代方向，并且在解决定位挑战方面非常成功，产生了精确的语义分割结果并在一定程度上恢复物体边界细节，远远超出现有方法。
&emsp;传统上，条件随机场(CRF)被用于平滑噪声分割图[23]，[31]。通常，这些模型耦合邻近节点，有利于对空间邻近像素分配相同的标签。定性地讲，这些短距离CRF的主要功能是减弱基于局部手工设计特征的弱分类器的错误预测。
&emsp;与这些弱分类器相比，现代DCNN体系结构如本文中使用的结构得到分数图和性质不同的语义标签预测。如图5所示，分数图通常相当平滑并产生均匀的分类结果。在这个情况下，使用短距离的CRFs可能是不好的，因为我们的目标应该是恢复详细的局部结构，而不是进一步平滑。使用局部CRF关联的反差灵敏势[23]可以潜在地改善定位，但仍然错过thin-structures/细小的结构，还需要解决高昂的离散优化问题。
&emsp;为了克服短距离CRF的这些限制，将我们的系统与[22]的全连接的CRF模型相结合。模型采用能量函数如下
![Formular 2](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/DeepLab2_Formular2.PNG)
&emsp;其中xx是像素的标签，用于单点势能θi(xi)=−logP(xi)θi(xi)=−log⁡P(xi) ，其中 P(xi)P(xi) 是由DCNN计算出的像素 ii 处的标签分配概率。成对势能具有相同的有效推断的形式，即当连接所有图像像素对 ii ，jj 时。具体如[22]所讲，我们使用以下表达式: 
![Formular 3](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/DeepLab2_Formular3.PNG)
&emsp;其中如果xi≠xjxi≠xj 那么μ(xi,xj)=1μ(xi,xj)=1 ，否则为0；其中，如在Potts模型中，意味着只有不同标签的节点会被惩罚。表达式的剩下部分使用两个不同特征空间中的高斯内核;第一个“双边”内核取决于像素位置(记作为pp)和RGB颜色(记作为II)，第二个内核仅依赖于像素位置。超参数σασα，σβσβ和σgammaσgamma控制高斯核的尺度。第一个内核强调具有相似颜色和位置的像素具有相似的标签，而第二个内核仅在强调平滑时考虑空间相似度。
&emsp;关键的是，这个模型适合于有效的近似概率推理[22]。在全分解的平均场近似值b(x)=Πibi(xi)b(x)=Πibi(xi)下的消息传递更新可以表示为双边空间中的高斯卷积。高维滤波算法[84]显著加速了这种计算，使得算法在实际中非常快速，在PASCAL VOC图像上使用[22]的开源方法，平均要少于0.5秒。
### 4. Experimental Results
&emsp;我们调整了ImageNet预训练的VGG-16或ResNet-101网络的模型权重，以[14]的步骤将它们应用在语义分割任务中。我们替换了最后一层的1000路ImageNet分类器，其分类器具有与我们任务中的语义类数(包括背景)。我们的损失函数是DCNN输出图(与原始图像相比，被8倍子采样)中每个空间位置的交叉熵项的总和。所有位置和标签在整体损失函数中均匀加权(除了未标记的像素被忽略)。我们的目标是真实值标签(8倍子采样)。我们通过[2]中的标准SGD算法来优化所有网络层中目标函数的权重。在设置CRF参数时，假设DCNN一元项是固定的，我们将DCNN和CRF训练阶段分开进行。
&emsp;我们在四个具有挑战性的数据集上评估提出的模型:PASCAL VOC 2012，PASCAL-Context，PASCAL-Person-Part和Cityscapes。我们首先报告了我们在PASCAL VOC 2012上的第一版[38]的主要结果，得到所有数据集的最新结果。
#### 4.1 PASCAL VOC 2012
数据集: 有20类前景目标类和1个背景类； 
原始数据集:包含1464(train), 1449(val), 1456(test)像素级的标注图像分别用于训练、验证和测试； 
增强数据集:由[85]提供的额外标注产生10582(training)的训练图像； 
性能评估:用21类的平均像素交叠率(mIoU)来衡量
##### 4.1.1会议版本结果
&emsp;采用ImageNet预训练的VGG-16网络，细节参考3.1。使用20个图像的patch，初始学习率为0.001(在分类器最后层为0.01)，每2000次迭代学习率乘以0.1。0.9的动量和0.0005的权重衰减。
&emsp;DCNN在增强集上训练微调后，我们以[22]的方式交叉验证了CRF参数。使用默认值w2=3w2=3 和σγ=3σγ=3，通过对来自valval的100个图像进行交叉验证来搜索w1w1，σα和σβ的最佳值。采用从粗略到精细的搜索方案。参数的初始搜索范围为w1∈[3:6]，σα∈[30:10:100]和σβ∈[3:6]，然后在第一轮最佳值周围优化搜索步长。使用10次平均场迭代。
![Table 1](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Table1.PNG)
&emsp;**视野和CRF**:表1中，公布了使用不同视野大小的DeepLab模型变体的实验，如3.1节所述，调整“fc6”层中的内核大小和多孔采样率rr。我们从直接使用VGG-16网络开始，用原始的7×7内核大小和r=4(因为我们对最后两个最大池化层没用stride)。该模型经CRF后产生67.64％的性能，但相对较慢(训练时每秒1.44张图像)。将内核大小减小到4×4，模型速度提高到了每秒2.9图像。我们已经尝试了两个具有较小(r=4)和较大(r=8)FOV的网络变体;后者表现更好。最后采用内核大小3×3，和更大的多孔采样率(r=12)，在层’fc6’和’fc7’中保留4,096个滤波器中的一个包含1024个滤波器的随机子集，使网络更瘦小。所得到的模型DeepLab-CRF-LargeFOV匹配直接采用VGG-16(7×7内核大小，r=4)的性能。同时，DeepLab-LargeFOV的速度是3.36倍，参数明显减少(20.5M而不是134.3M)。
&emsp;CRF大大提高了所有模型变体的性能，平均IOU增长了3-5％。
##### 4.1.2 会议之后的改进版本
&emsp;第一版[38]发布后，我们对模型进行了三个主要的改进:(1)训练过程中的不同学习策略(2)多孔空间金字塔池化(3)更深的网络和多尺度处理的应用。
![Table 2](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Table2.PNG)
&emsp;**学习率**:在训练DeepLab-LargeFOV时，采用了不同的学习率策略。与[86]类似，我们还发现，采用“poly”策略：
![Poly](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Poly.PNG)
比“step”学习率(以固定步长降低学习率)更有效。 
如表2所示，采用“poly”(power=0.9)并使用相同的批量大小和相同的训练迭代次数，比采用“step”策略的性能提高了1.17％。固定批量大小并将训练迭代次数增加到10K，性能提高到了64.90％(增长1.48％);然而，由于更多的训练迭代，总训练时间增加。然后我们将批量大小降至10，对比性能发现仍然性能保持不变(64.90％比64.71％)。最后，我们采用批量大小=10以及20K次的迭代，以保持与以前的“step”策略相似的训练时间。令人惊讶的是，我们在验证集上表现为65.88％(相比“step”提高了3.63％)，在测试集上为67.7％，而DeepLab-LargeFOV在CRF之前采用“step”的设置仅达到65.1％的性能。我们对本文其余部分报告的所有实验均采用“poly”策略。
![Table 3](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Table3.PNG)
&emsp;**多孔空间金字塔池化**:用3.1.1节中描述的提议-多孔 Spatial Pyramid Pooling(ASPP)做实验。如图7所示，VGG-16的ASPP采用多个并行fc6-fc7-fc8分支。它们都使用3×3内核，但在’fc6’中的不同的速度r以捕获不同大小的物体。在表3公布了几种配置的结果:(1)我们的基线LargeFOV模型，r=12的单个分支(2)ASPP-S，具有四个分支和较小的多孔比例(r={2,4，8,12})及(3)ASPP-L，具有四个分支和较大的比例(r={6,12,18,24})。对于每个变体，我们公布了CRF前后的结果。如表所示，ASPP-S在CRF前的基线LargeFOV上提高了1.22％。然而，在CRF之后，LargeFOV和ASPP-S表现相似。另一方面，ASPP-L在基线LargeFOV使用CRF前后都得到了一致的改进。我们在测试集上评估ASPP-L+CRF模型，达到72.6％。图8展示了不同方案的效果。 
![Figure 7](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Figure7.PNG)
&emsp;**更深的网络和多尺度处理**:用残差网络ResNet-101[11]构建DeepLab。类似对VGG-16网络所做的，通过多孔卷积重新使用ResNet-101，如3.1节所述。此外[17]，[18]，[39]，[40]，[58]，[59]，[62]采用了其他几个特征:(1)多尺度输入:分别以scale={0.5,0.75,1}scale={0.5,0.75,1}的DCNN图像馈送，分别对每个位置进行尺度的最大响应来整合其分数图。(2)在MS-COCO上预先训练的模型[87]。(3)通过在训练期间随机缩放输入图像(从0.5到1.5)来增加数据。在表4中，我们评估LargeFOV和多孔空间金字塔池(ASPP)在验证集中每个因素如何影响性能的。采用ResNet-101显著提高了DeepLab性能(例如，我们最简单的ResNet-101型号达到68.72％，而在CRF之前，我们的DeepLab-LargeFOV-VGG-16变体只有65.76％)。多尺度融合[17]提高了2.55％，而MS-COCO上的模型预测还增加了2.01％。训练期间的数据增加是有效的(约有1.6％的改善)。使用LargeFOV(在ResNet顶部添加一个多孔卷积层，3×33×3内核和rate=12rate=12)是有益的(约0.6％的改进)。通过多孔空间金字塔池(ASPP)进一步提高了0.8％。通过密集CRF后处理的最佳模型表现为77.69％。
&emsp;**定性结果**: DeepLab在CRF前后的结果(我们最好的模型变体)的定性可视化对比。在CRF之前，DeepLab获得的可视化结果已经产生了优异的分割结果，而采用CRF后可以通过消除假阳性和优化物体边界进一步提高性能。
![Table 5](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Table5.PNG)
&emsp;**测试结果**:将最终的最佳模型的结果提交给官方服务器，获得了79.7％的测试集性能，如**表5**所示。该模型基本上胜过以前的DeepLab变体(例如，具有VGG-16网络的DeepLab-LargeFOV)，目前是PASCAL VOC 2012分段排行榜中性能最佳的方法。 
![Figure 9](https://paper-reading-1258239805.cos.ap-chengdu.myqcloud.com/Deeplab2_Figure9.PNG)
&emsp;**VGG-16 vs. ResNet-101**:我们发现，基于ResNet-101 [11]的DeepLab比采用VGG-16[4]产生了更好的物体边界分割结果，如图9所示。我们认为ResNet-101的身份映射[94]具有与超列特征相似的效果[21]，它利用中间层的特征来更好地定位边界。在“trimap”(沿物体边界的窄带)[22]，[31]内进一步量化图10中的这种效果。如图所示，在CRF之前采用ResNet-101与使用VGG-16结合CRF的物体边界有几乎相同的精度。用CRF后处理ResNet-101结果进一步提高了分割结果。 
***
代码相关：
 poly学习率策略：
``` python
 def lr_schedule(epoch):
	power = 0.9
	maxepoch = 100
	return pow(1-epoch/maxepoch,power)
 ```
 
 损失函数：
``` python
def log_loss(self, y_true, y_pred):
	'''
	Compute the softmax log loss.

	Arguments:
		y_true (nD tensor): size(batch,width,heght,nclass)
		y_pred (nD tensor): size(batch,width,heght,nclass)
	Returns:
		The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor
		of shape (batch,).
	'''
	shape = K.shape(y_true)
	y_true = K.reshape(y_true,(-1,shape[1]*shape[2],shape[3]))
	y_pred = K.reshape(y_pred,(-1,shape[1]*shape[2],shape[3]))
	y_pred = tf.maximum(y_pred, 1e-15)

	log_loss = tf.reduce_sum(-tf.reduce_sum(y_true * tf.log(y_pred), axis=-1), axis=-1)
	return log_loss
 ```

VGG_16为基础网络
```python
from keras.models import *
from keras.layers import *
from keras.applications.vgg16 import VGG16
import tensorflow as tf
def DeeplabV2(input_shape=(224,224,3),apply_softmax=True,classes=2):
	base_model = VGG16(include_top=None, pooling=None, weights='imagenet', input_shape = input_shape)
	img_input = base_model.input

	f4 = base_model.get_layer('block4_conv3').output    #block4_conv3  (28,28)
	h = MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(f4)

	# Block 5
	h = Conv2D(512, (3, 3), dilation_rate=(2, 2), activation='relu', name='conv5_1',padding="same")(h)
	h = Conv2D(512, (3, 3), dilation_rate=(2, 2), activation='relu', name='conv5_2',padding="same")(h)
	h = Conv2D(512, (3, 3), dilation_rate=(2, 2), activation='relu', name='conv5_3',padding="same")(h)
	p5 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(h)

	# hole = 6
	b1 = Conv2D(1024, (3, 3), dilation_rate=(6, 6), activation='relu', name='fc6_1',padding="same")(p5)
	b1 = Dropout(0.5)(b1)
	b1 = Conv2D(1024, (1, 1), activation='relu', name='fc7_1',padding="same")(b1)
	b1 = Dropout(0.5)(b1)
	b1 = Conv2D(21, (1, 1), activation='relu', name='fc8_voc12_1',padding="same")(b1)

	# hole = 12
	b2 = Conv2D(1024, (3, 3), dilation_rate=(12, 12), activation='relu', name='fc6_2',padding="same")(p5)
	b2 = Dropout(0.5)(b2)
	b2 = Conv2D(1024, (1, 1), activation='relu', name='fc7_2',padding="same")(b2)
	b2 = Dropout(0.5)(b2)
	b2 = Conv2D(21, (1, 1), activation='relu', name='fc8_voc12_2',padding="same")(b2)

	# hole = 18
	b3 = Conv2D(1024, (3, 3), dilation_rate=(18, 18), activation='relu', name='fc6_3',padding="same")(p5)
	b3 = Dropout(0.5)(b3)
	b3 = Conv2D(1024, (1, 1), activation='relu', name='fc7_3',padding="same")(b3)
	b3 = Dropout(0.5)(b3)
	b3 = Conv2D(21, (1, 1), activation='relu', name='fc8_voc12_3',padding="same")(b3)

	# hole = 24
	b4 = Conv2D(1024, (3, 3), dilation_rate=(24, 24), activation='relu', name='fc6_4',padding="same")(p5)
	b4 = Dropout(0.5)(b4)
	b4 = Conv2D(1024, (1, 1), activation='relu', name='fc7_4',padding="same")(b4)
	b4 = Dropout(0.5)(b4)
	b4 = Conv2D(21, (1, 1), activation='relu', name='fc8_voc12_4',padding="same")(b4)
	add1 = Add()([b1,b2,b3,b4])
	o = Conv2DTranspose(1,kernel_size=(16,16), strides=(8, 8),padding="same", use_bias=False, activation='sigmoid')(add1)

	if apply_softmax:
		o = Conv2DTranspose(classes, kernel_size=(16, 16),strides=(8, 8), padding='same',use_bias=False,activation='softmax')(add1)
	model = Model(inputs=img_input, outputs=o)
	return model

if __name__ == '__main__':
	model = DeeplabV2(input_shape = (224,224,3), apply_softmax=False,classes=2)
	model.summary()
	print('load successfully')
 ```







